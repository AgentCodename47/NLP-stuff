{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfpYbIaDfaNsirmgL/qFIu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgentCodename47/NLP-stuff/blob/main/bigram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "y8yL9TKWAI4G",
        "outputId": "b822abd9-472b-4736-d45f-fc20b00c8ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram probabilities for previous word '<s>':\n",
            "  P('the' | '<s>') = 2/3\n",
            "  P('these' | '<s>') = 1/3\n",
            "Bigram probabilities for previous word 'the':\n",
            "  P('arabian' | 'the') = 2/5\n",
            "  P('fairy' | 'the') = 1/5\n",
            "  P('east' | 'the') = 1/5\n",
            "  P('stories' | 'the') = 1/5\n",
            "Bigram probabilities for previous word 'arabian':\n",
            "  P('knights' | 'arabian') = 1/1\n",
            "Bigram probabilities for previous word 'knights':\n",
            "  P('</s>' | 'knights') = 1/2\n",
            "  P('are' | 'knights') = 1/2\n",
            "Bigram probabilities for previous word 'these':\n",
            "  P('are' | 'these') = 1/1\n",
            "Bigram probabilities for previous word 'are':\n",
            "  P('the' | 'are') = 1/2\n",
            "  P('translated' | 'are') = 1/2\n",
            "Bigram probabilities for previous word 'fairy':\n",
            "  P('tales' | 'fairy') = 1/1\n",
            "Bigram probabilities for previous word 'tales':\n",
            "  P('of' | 'tales') = 1/1\n",
            "Bigram probabilities for previous word 'of':\n",
            "  P('the' | 'of') = 1/1\n",
            "Bigram probabilities for previous word 'east':\n",
            "  P('</s>' | 'east') = 1/1\n",
            "Bigram probabilities for previous word 'stories':\n",
            "  P('of' | 'stories') = 1/1\n",
            "Bigram probabilities for previous word 'translated':\n",
            "  P('in' | 'translated') = 1/1\n",
            "Bigram probabilities for previous word 'in':\n",
            "  P('many' | 'in') = 1/1\n",
            "Bigram probabilities for previous word 'many':\n",
            "  P('languages' | 'many') = 1/1\n",
            "Bigram probabilities for previous word 'languages':\n",
            "  P('</s>' | 'languages') = 1/1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a7780639a910>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mFraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigram_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprev_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madd_k\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mbigram_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P('{}' | '{}') = {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probability of the sentence '{}': {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '<'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from fractions import Fraction\n",
        "\n",
        "# Define the training set\n",
        "training_set = [\"The Arabian knights\", \"These are the fairy tales of the east\", \"The stories of the Arabian knights are translated in many languages\"]\n",
        "\n",
        "# Define the target sentence\n",
        "target_sentence = \"<s> The Arabian knights are the fairy tales of the east </s>\"\n",
        "\n",
        "# Define the vocabulary\n",
        "vocab = set()\n",
        "for sentence in training_set:\n",
        "    tokens = nltk.word_tokenize(sentence.lower())\n",
        "    vocab.update(tokens)\n",
        "# Remove the '<s>' and '</s>' tokens from the vocabulary\n",
        "vocab.discard('<s>')\n",
        "vocab.discard('</s>')\n",
        "\n",
        "# Calculate the bigram counts\n",
        "bigram_counts = Counter()\n",
        "for sentence in training_set:\n",
        "    tokens = nltk.word_tokenize(sentence.lower())\n",
        "    bigrams = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
        "    bigram_counts.update(bigrams)\n",
        "\n",
        "# Calculate the bigram probabilities\n",
        "bigram_probs = {}\n",
        "for bigram, count in bigram_counts.items():\n",
        "    prev_word = bigram[0]\n",
        "    if prev_word not in bigram_probs:\n",
        "        bigram_probs[prev_word] = {}\n",
        "    total_count = sum(count for _, count in bigram_counts.items() if _[0] == prev_word)\n",
        "    prob = Fraction(count, total_count)\n",
        "    bigram_probs[prev_word][bigram[1]] = prob\n",
        "\n",
        "# Print the bigram probabilities\n",
        "for prev_word in bigram_probs:\n",
        "    print(\"Bigram probabilities for previous word '{}':\".format(prev_word))\n",
        "    for next_word, prob in bigram_probs[prev_word].items():\n",
        "        print(\"  P('{}' | '{}') = {}/{}\".format(next_word, prev_word, prob.numerator, prob.denominator))\n",
        "\n",
        "# Apply add-1 smoothing\n",
        "add_k = 1\n",
        "vocab_size = len(vocab)\n",
        "for prev_word in bigram_probs:\n",
        "    for next_word in vocab:\n",
        "        if next_word not in bigram_probs[prev_word]:\n",
        "            bigram_probs[prev_word][next_word] = Fraction(add_k, sum(count for _, count in bigram_counts.items() if _[0] == prev_word) + add_k * vocab_size)\n",
        "\n",
        "# Calculate the probability of the target sentence\n",
        "tokens = nltk.word_tokenize(target_sentence.lower())\n",
        "bigrams = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
        "prob = Fraction(1, 1)\n",
        "for bigram in bigrams:\n",
        "    prev_word = bigram[0]\n",
        "    next_word = bigram[1]\n",
        "    if prev_word not in bigram_probs:\n",
        "        # Handle the case when prev_word is not present in the bigram_probs dictionary\n",
        "        prob *= Fraction(add_k, sum(count for _, count in bigram_counts.items() if _[0] == prev_word) + add_k * vocab_size)\n",
        "    else:\n",
        "        prob *= bigram_probs[prev_word][next_word]\n",
        "        print(\"P('{}' | '{}') = {}/{}\".format(next_word, prev_word, bigram_probs[prev_word].numerator, bigram_probs[prev_word][next_word].denominator))\n",
        "print(\"Probability of the sentence '{}': {}/{}\".format(target_sentence, prob.numerator, prob.denominator))\n"
      ]
    }
  ]
}